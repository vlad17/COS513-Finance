\subsection{Improving the Prediction Models}
We have tried different window sizes for both training and testing as well as moving the windows instead of fixing them. We also tried using PCA for LM and ARIMA but found that using Lasso once the orders of AR and MA are determined to be more efficient. Our binary accuracy results are on-par with the state-of-the-art ones. However, it is clear that our prediction, for the simple log return data, is most from the time series lags itself and not GDELT. The sparser our usage of GDELT is, the higher our testing results are. Therefore, we have to either analyze different time series (discussed later in this section) or try to estimate different features of the time series such as extreme values or peaks in the log return. For example, we could work on a logistic regression that classifies days in the top $10\%$ in terms of log return value, as "high", days in the bottom $10\%$ as low and the rest as "medium". In this case, we would be doing two classifiers, one on the "high" and one on the "low" in a similar fashion to exemplar SVM, where we train the high/low with so many negative examples that it has a higher certainty where predicting a peak. Our final result would indicate whether there are peaks (high or low) or if the trading activity is rather normal (medium) or if our results conflict (we get positive for both high and low). Only in the first case where we are sure about one kind of the peaks would we consider the prediction for our trading strategy.

\subsection{Infinite Gaussian Mixture Model}
For clustering news events we have relatively little information for deciding how many clusters there should be. In our $K$-means model, we do a parameter search for values of $K$ from 10 to 5000 on a logarithmic scale. Ideally we would be able to use an infinite Gaussian mixture model that takes in a hyperparameter for a clustering coefficient and automatically determines the number of clusters and therefore remove the need for this imprecise parameter search. 

We attempted using a Dirichlet Process GMM but the implementation we attempted to use was intractable given our computing power. We may attempt to optimize the parameters of the DPGMM in the future or work on limiting the model's training set further. 

\subsection{Filtering GDELT by Commodity}
Up to now, we have been sampling the entire GDELT dataset for news events in order to predict the movement in commodity prices (e.g. silver). Since GDELT contains a large variety of news topics, the majority of the news articles within each day are not related to the specific commodity that we study. A more accurate way to predict the commodity prices would be to implement a filtering pipeline that removes irrelevant news items. This can be achived with natural language processing by selecting news articles that have keywords such as ``silver" and ``commodity". This approach should improve our prediction accuracy because it would remove a significant portion of the noise in GDELT.


