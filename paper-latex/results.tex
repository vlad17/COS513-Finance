
\subsection{Clustering}

We manually inspect our clusters for both high similary of articles within clusters and ow similarity of articles between clusters. Since we do not have a gold standard for news event labels, we cannot calculate values such as true positives and true negatives. We present some qualitative results.

\textbf{First ten articles of Cluster 0:}  
\begin{enumerate}
\item Retrial of 3 Al-Jazeera journalists (Egypt)
\item Protests for seizing of hospital accounts (DR Congo)
\item Waterloo homicide arrest (England)
\item Taliban vs Afghan army (Afghanistan)
\item \$20 million funding for Wilmington pharmacy (Delaware)
\item Tiger farms violate Endangered Species Law (China)
\item DeSoto corruption trial (Mississippi)
\item Six die at David Owuor's Nakuru crusade (Kenya)
\item Temp workers fight for wages (Chicago)
\item Grant of bail to Lakhvi, mastermind of Mumbai attack (Pakistan)
\end{enumerate}

As can be seen, purity for cluster 0 is fairly high, with seven of ten articles dealing with judicial decisions. In general, most clusters appear to have reasonable purity, but there is also high similarity between clusters. For example, articles similar to the first article in cluster 0 also appear in cluster 5. Cluster 5 is similar to cluster 0 in that they both deal with judicial decisions.\\

\noindent A search for the name of the mastermind of the Mumbai attacks, Zakiur Rehman Lakhvi, returned results in 69 of 1000 clusters, which indicates we have much more cross-cluster similarity than we'd like. But this is a more preferable problem to have than a lack of intra-cluster similarity. Our eyeball test can only evaluate cluster quality based on keywords such as actor names and locations and general topics. Its possible that Lakhvi can appear in 69 clusters because the other metrics articles are clustered on, such as tone or the importance features, which an eyeball test cannot detect, do in fact separate Lakhvi related articles into that many clusters.

\subsection{Regression}



\begin{figure}[h!]
	\caption{Accuracy for various test sizes}
	\centering
	\includegraphics[width=0.5\textwidth]{results/acc.jpg}
\end{figure}

\begin{figure}[h!]
	\caption{Mean aboslute error for various test sizes}
	\includegraphics[width=0.5\textwidth]{results/mae.jpg}
\end{figure}

\begin{figure}[h!]
	\caption{Accuracy for various sparsity}
	\includegraphics[width=0.5\textwidth]{results/acc2.jpg}
\end{figure}

\begin{figure}[h!]
	\caption{Mean abolsute error for various sparsity}
	\includegraphics[width=0.5\textwidth]{results/mae2.jpg}
\end{figure}

\begin{figure}[h!]
	\caption{Comparison of log returns for sparsity = 0.1, training size = 400}
	\includegraphics[width=0.5\textwidth]{results/prediction.jpg}
\end{figure}


TODO(Sean): test err, $R^2$, residuals plot, residuals normality test?
acc/mae
Accuracy across decreasing testing size(columns) and decreasing training size(row)
MAE(mean absolute error) same labelling
Fixed windw train last 500 test on 200
acc2/mae2
decreasing training size(columns) decreasing sparsity (row)
mae2(3,3) is lowest
moving windows train 200 test 

pred vs actual

best adj r2 is 0.66

